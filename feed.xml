<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-09-17T14:39:35-04:00</updated><id>/feed.xml</id><title type="html">Gabe Schumm</title><subtitle>Computational physicist dipping his toes into sports analytics</subtitle><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><entry><title type="html">Simulating MLB Pitch Tracjetories</title><link href="/pitching-primer/" rel="alternate" type="text/html" title="Simulating MLB Pitch Tracjetories" /><published>2025-09-07T00:00:00-04:00</published><updated>2025-09-07T00:00:00-04:00</updated><id>/pitch_viz</id><content type="html" xml:base="/pitching-primer/"><![CDATA[<figure style="float: right;
               margin: 0 0 1em 1em;
               border: 2px solid #ddd;
               border-radius: 8px;
               background-color: #fff;
               padding: 8px;
               text-align: center;">

  <iframe src="/assets/js/traj/fbd.html" width="250" height="250" frameborder="0" scrolling="no" style="border: none;">
  </iframe>

  <figcaption style="font-style: italic; color: #555; font-size: 16px; margin-top: 5px; text-align: center;">
    Direction of forces acting <br /> on a typical two-seam fastball.
  </figcaption>
</figure>

<p>My pitch visualizer draws heavily from a series of papers by <a href="https://baseball.physics.illinois.edu/">Professor Alan Nathan</a>
of the University of Illinois. The core idea is to model the forces acting on a spinning baseball, gravity, drag, and lift (the latter broken into Magnus and side forces), and use Newton’s Second Law to derive a set of <em>equations of motion</em>. The directions of these forces are fully determined by the velocity \(\vec{v}\) and spin-axis \(\vec{\omega}\), shown in the plot on the right for a typical two-seam fastball. The magnitudes of the drag, Magnus, and side forces, however, must be extracted from the kinematic information provided by Statcast.</p>

<p>This procedure was first described by Nathan in his <a href="https://baseball.physics.illinois.edu/trackman/spinaxis.pdf">2015 paper</a>, which was subsequently updated in 2018 and 2020. In this earlier iteration, the spin-axis provided by Statcast was not actually measured but instead inferred by the pitch motion, and a result, the Magnus and side forces could not be separated. With the implementation of the Hawk-Eye camera system in 2020, Statcast began measuring the true spin-axis directly, which is what made clear that Magnus was not the only lift force at play. This was first explored in depth by Nathan, along with Professor Barton Smith and Harry Pavlidis, in their 2020 paper <a href="https://www.baseballprospectus.com/news/article/62912/not-just-about-magnus-anymore/">Not Just About Magnus Anymore</a>, which laid out the groundwork for what is now well understood as the <strong>seam-shifted wake (SSW)</strong> effect.</p>

<figure class="float-right-figure">
  <img src="/assets/pitch_traj/coords.png" alt="coords" />
   <figcaption>Statcast coordinate system.</figcaption>
</figure>

<p>In <a href="https://baseball.physics.illinois.edu/HawkeyeSpinAnalysis-MLB-Part%20II-v2-public.pdf">2021</a>, Nathan introduced a method to model SSW—the side lift force—using the true spin-axis. However, this approach still required some approximations, since Statcast still only provides two components of the 3D spin-axis (\(\omega_x\) and \(\omega_z\)). But with the release of the active spin (i.e. spin efficiency) <a href="https://baseballsavant.mlb.com/leaderboard/spin-direction-pitches">leaderboard</a>, the third component of the spin-axis can now be approximated for any given pitch in a pitcher’s arsenal pitch, as described in Nathan’s <a href="https://baseball.physics.illinois.edu/HawkeyeAveSpinComponents.pdf">2024 paper</a>. This equips us with all the information needed to simulate the trajectory of any pitch in Statcast’s database, and to directly explore how changing spin-rate, spin-axis, or spin efficiency affect pitch motion.</p>

<p>I plan on posting a more in-depth article on the details behind all of this soon, but I highly encourage the curious reader to check out Nathan’s papers (paper <a href="https://baseball.physics.illinois.edu/trackman/spinaxis.pdf">1</a>, <a href="https://baseball.physics.illinois.edu/HawkeyeSpinAnalysis-MLB-Part%20II-v2-public.pdf">2</a>, and <a href="https://baseball.physics.illinois.edu/HawkeyeAveSpinComponents.pdf">3</a>) and <a href="https://www.baseballprospectus.com/news/article/62912/not-just-about-magnus-anymore/">Not Just About Magnus Anymore</a>.</p>

\[\sim\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[A short primer on my pitch visualization tool.]]></summary></entry><entry><title type="html">NBA Matchup Predictor</title><link href="/matchup_pred/" rel="alternate" type="text/html" title="NBA Matchup Predictor" /><published>2025-08-01T00:00:00-04:00</published><updated>2025-08-01T00:00:00-04:00</updated><id>/win_projector</id><content type="html" xml:base="/matchup_pred/"><![CDATA[<p>Predicting the result of a matchup in any sport is a challenge. Sportsbooks use sophisticated models trained on large quantities of historic data that are frequently updated in real time information like consider injury reports and market behavior. But setting a money line or point spread essentially boils down to a data science problem, and with <a href="/shot_chart/">access to nearly 30 years of NBA data</a>, one that I can try to tackle.</p>

<p>The goal of this project is to explore the ability of different ML models to predict the outcome of NBA game based solely on data from each team’s games played before the matchup. I’ll explore various aspects of feature engineering, compare models of varying complexity, and attempt to distill the key factors that can swing a matchup. But before we start, we must establish baselines to compare our models against.</p>

<h2 id="some-notes-about-the-dataset">Some Notes About the Dataset</h2>
<ul>
  <li>Each season from contains \(\left(82 \times 30\right)/2\) ~ 1,200 matchups, which are stored are as ~2,400 total games (one row for each team in each game).</li>
  <li>There are 28 total seasons in the dataset, spanning 1997-98 to 2024-25. Of these 28, I have omitted four:
    <ul>
      <li>The two lockout seasons, 1998-99 and 2011-12</li>
      <li>The two seasons most affected by COVID, 2019-20 and 2020-21</li>
    </ul>
  </li>
</ul>

<h2 id="bayes-rate">Bayes Rate</h2>
<p>After defining a ML task, one must determine the best possible performance they should expect to achieve with their model. This expert level performance is what is called the <em>Bayes rate</em> in the context of Bayesian statistics and can be thought of as the inherent uncertainty baked into the problem. For the task of predicting NBA matchups, a sportsbook’s money line can act as the expert’s prediction — their models are sure to be stronger than ours, so they should provide a good benchmark.</p>

<p>Historic betting data is more difficult to scrape from the web than the stats provided by stats.nba.com. Many of site that host this data keep it behind paywalls, and the open data that is available is not exhaustive. Thankfully, a kind patron of one of these subscription-based sites <a href="https://www.reddit.com/r/sportsbook/comments/rslmm3/database_of_nba_spreadsovers_and_almost_all_box/">published</a> their betting history data on Kaggle for public use. It was stored in a single, flat CSV, and didn’t contain many of the columns used as primary keys in my SQL database, but after some wrangling and cleaning, I was able to insert this dataset into my database for use in this project.</p>

<p>Since the 2007-08 season, the earliest the betting history dataset going back to, the team favored by the sportsbooks (i.e. the team with a negative spread) has won <strong>68.7%</strong> of games. This defines our Bayes rate — if my model can predict wins with an accuracy above 68.7%, I will deem this project a success. A confusion matrix provides a nice visual for how the sportsbooks’ predictions perform for each class (wins and losses).</p>

<figure class="framed-figure" style="width: 40%;">
  <img src="/assets/win_projector/sportsbook.png" alt="season_four" />
  <figcaption>Expert baseline model based on sportsbooks' predictions.</figcaption>
</figure>

<h2 id="simplest-win-projector--home-team">Simplest Win Projector — Home Team</h2>
<p>It is also helpful to define a minimum performance baseline that we are sure our model will perform better than. A simple one would rely only on home court advantage — without knowing any details about a matchup, it’s not a bad guess to predict that the home team will win. And this is borne out in our dataset: across the ~40,000 total regular season matchups, the home team won 58.8% of the time. We can use this to inform our lower baseline model and define a <em>Bayesian prior</em> — before considering any team specific data, what is the likelihood of observing a win given our real-world, historical knowledge of the NBA. Our simple model will simply predict</p>

\[P(\text{A beats B}) = 
\begin{cases}
0.588 &amp; \text{if A is home team} \\
1-0.588 = 0.411 &amp; \text{if B is home team} 
\end{cases}\]

<p>and assign a win to the home team every single time.</p>

<p>Since our models predict win probabilities, not just W/L, to score our them we will use three metrics:</p>
<ul>
  <li>Accuracy score (i.e. # correct predictions / # total predictions)</li>
  <li>Cross-entropy loss (i.e. negative log-likelihood)</li>
  <li>Confusion Matrix (i.e. breakdown of true/false positives/negatives)</li>
</ul>

<p>Since our dataset is perfectly balanced between the two classes of outcomes, the accuracy score provides a simple and descriptive way to evaluate how well our model is at performing the task at hand (predicting matchups). The cross-entropy loss is a gauge of how well-calibrated our classifier is, penalizing confident wrong predictions, and providing better feedback for a model that predicts the probability of an event occurring. And finally, the confusion matrix shown, as shown above. Since our data has exactly as many true wins as true losses, we should expect similar behavior within each true outcome.</p>

<p>Our three metrics for this simple model are displayed below.</p>

<!-- ![simple](/assets/win_projector/simple.png) -->

<figure class="framed-figure">
  <img src="/assets/win_projector/simplest.png" alt="season_four" />
  <figcaption>Baseline model based purely on home court advantage.</figcaption>
</figure>

<p>The accuracy of this model is 59%, as expected, and because it predicts a flat win probability, it performs identically across both wins and losses. The loss is large though, relatively speaking — a completely uninformed model that predicts wins and losses with a constant probability of 50% has a cross-entropy loss of ~0.693. To provide a scale to compare the loss for our future models, we will use the value from this baseline model as the max loss we aim to improve upon. We can now begin to provide our model with team context.</p>

<h2 id="season-to-date-win">Season-to-Date Win%</h2>
<p>A simple step up from our baseline model is to predict matchups based purely on team win % at the time of game. If Team A has a higher win % than Team B, then our model will predict that Team A will win the matchup, and vice versa. A
softer model may instead produce the probability that Team A/B wins, using the simple <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry (BT) model</a>:</p>

\[P(\text{A beats B}) = \frac{\text{W}\%_{\rm A}}{\text{W}\%_{\rm A} + \text{W}\%_{\rm B}},\]

<p>where our classifier would then assign a win to Team A if \(P(\text{A beats B}) &gt; 0.5\). We expect that a teams season-to-date win percentage will need time to stabilize, so I will predict matchups only after each team has played 10 games (chosen somewhat arbitrarily here, but explored in more detail below).</p>

<figure class="framed-figure">
  <img src="/assets/win_projector/simple.png" alt="season_four" />
  <figcaption>Bradley-Terry model based on season-to-date win percentage.</figcaption>
</figure>

<p>This simple win model has an overall accuracy of 65%, already not so bad. The loss has been reduced slightly but can certainly be improved further with stronger feature engineering and a more complex model architecture.</p>

<h2 id="four-factor---running-average">Four Factor - Running Average</h2>
<p><a href="http://www.basketballonpaper.com/">Dean Oliver</a>, one of the pioneers of basketball analytics, developed the “four factors” as a simple and powerful method to predict basketball success. The four factors aim to isolate the most impactful contributors to winning: shooting, turnovers, rebounding, and free throws. They can be broken down into eight stats, four for offense and four for defense:</p>

<!-- ![eos](/assets/win_projector/eos_four.png){: style="max-width: 48%; height: auto; float: left; padding-right: 50px"} -->

<figure class="float-left-figure">
  <img src="/assets/win_projector/eos_four.png" alt="eos" />
  <figcaption>Predicting end-of-season win percentage using linear model trained on the four factors. Data from 1997-98 to 2024-25 NBA seasons.</figcaption>
</figure>

<ul>
  <li>Effective Field Goal Percentage</li>
  <li>Turnover Percentage</li>
  <li>Offensive Rebound Percentage</li>
  <li>Free Throw Rate</li>
  <li>Opponent’s Effective Field Goal Percentage</li>
  <li>Opponent’s Turnover Percentage</li>
  <li>Defensive Rebound Percentage</li>
  <li>Opponent’s Free Throw Rate</li>
</ul>

<p>It’s well known that NBA team performance in these eight statistics correlates very well with end of season win percentage. Our linear regression confirms this, with all features exhibiting high statistical significance (\(p \ll 0.05\)). This begs the question, do these stats hold the same predictive power on a game-by-game basis?</p>

<p>To test this, I used the team box score data scraped stats.nba.com of (see my post on the <a href="/shot_chart/">evolution of the NBA shot chart</a> for details on this process). This data set has all the information I need to calculate the rolling average four factor statistics at any point in a season for any time; these will be the features I use in a generalized linear model (i.e. logistic regression model) to predict matchup outcomes. The feature matrix is illustrated below:</p>

<table>
  <thead>
    <tr>
      <th>Matchup</th>
      <th>Team A Four Factors (Off.)</th>
      <th>Team A Four Factors (Def.)</th>
      <th>Team B Four Factors (Off.)</th>
      <th>Team B Four Factors (Def.)</th>
      <th>Context</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><span style="font-size: 12px;">\(A_1\) vs. \(B_1\)</span></td>
      <td>▇▁▅▃</td>
      <td>▆▂▃▅</td>
      <td>▇▆▃▁</td>
      <td>▄▂█▁</td>
      <td>▄▂█▁</td>
    </tr>
    <tr>
      <td><span style="font-size: 12px;">\(A_2\) vs. \(B_2\)</span></td>
      <td>▃▆▁▇</td>
      <td>▄▁▆▃</td>
      <td>▅▁▄▇</td>
      <td>▂▇▃▁</td>
      <td>▄▂█▁</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td><span style="font-size: 12px;">\(A_N\) vs. \(B_N\)</span></td>
      <td>▃▆▁▇</td>
      <td>▄▁▆▃</td>
      <td>▅▁▄▇</td>
      <td>▂▇▃▁</td>
      <td>▄▂█▁</td>
    </tr>
  </tbody>
</table>

<p>Each row represents a single matchup, and the columns correspond to the 2 \(\times\) 8 four factor stats for each team, averaged over all games within the season leading up to the matchup. I also add some “context” stats that I expect will help inform the W/L decision: season-to-date win percentage for each team, as used in the previous model, whether Team A or B is at home (one-hot encoded), and how many days of rest both Team A and Team B have. In total, there were ~40,000 rows of data.</p>

<p>Generating this feature matrix required some simple processing of the basic box score stats, a self-join on <code class="language-plaintext highlighter-rouge">game_id</code> with a filter to remove duplicates (to calculate the defensive four factors), a running average aggregation to generate season-to-date statistics, and another self-join on <code class="language-plaintext highlighter-rouge">game_id</code> to combine the aggregated data for both teams in each matchup. The initial query of the box score data was done using SQL, but much of the heavy lifting was done in Python, using <code class="language-plaintext highlighter-rouge">pandas</code>.</p>

<p>Here are the results of the trained logistic regression model on the test data (75%/25% split). Again, all features were statistically significant (\(p \ll 0.05\)).</p>

<figure class="framed-figure">
  <img src="/assets/win_projector/four_factor.png" alt="season_four" />
  <figcaption>Logistic regression model based on season-to-date four factor stats (10 game cut off).</figcaption>
</figure>

<p>While the accuracy improved only modestly, the cross-entropy loss has reduced more substantially, indicating that this model predicts wins and losses more confidently than the simple win percentage-based BT model. It is interesting to note that the form of the BT model can be simply related to the sigmoid function used to generate probabilities in logistic regression, if team strength is instead reparametrized as an exponential. In this case, team win percentage is not the most appropriate metric to use, and something like the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">ELO rating</a> common in chess would be better. <a href="https://www.reddit.com/r/nba/comments/1j2do9p/i_calculated_the_nba_elo_ratings_for_202425/">Calculating ELO rating for NBA teams</a> would be an interesting extension to this simple BT model.</p>

<!-- ![eos](/assets/win_projector/stabilize.png){: style="max-width: 48%; height: auto; float: left; padding-right: 50px"} -->

<figure class="float-left-figure">
  <img src="/assets/win_projector/stabilize.png" alt="eos_four" />
  <figcaption>Stabilization of the running average four factor stats, as represented as the mean deviation across all teams in the data set from their values at end-of-season.</figcaption>
</figure>

<p>Just as with win percentage model, I imposed the constraint that each team must have played at lease 10 games to be part of the data set, to give some time for the four factor stats to stabilize. Empirically, it appears that the four factor stats stabilize at different rates (see plot on left). But after around 10 games, on average, each stat is within 10% of its end of season value, implying some level of stabilization. Choosing a larger cut off could improve the model, but at the cost of being able to predict fewer outcomes within a given season. We can explore this balance using cross-validation.</p>

<p><br /></p>

<h3 id="choosing-a-minimum-games-played-cutoff">Choosing a Minimum Games Played Cutoff</h3>

<figure class="center-figure">
  <img src="/assets/win_projector/running_avg_cutoff_bts.png" alt="cut off" />
  <figcaption>Performance of model trained on data using different minimum games played cut offs.</figcaption>
</figure>

<p>Two choose an optimal minimum game cut off, I further split the training data into a smaller training set and validation set (75%/25% split, once again). I trained the model on subsets of the training data set, pruning to keep only matchups where both teams played a set minimum number of games, and evaluated the result on the validation data. After 60 repeats, I plotted the average training/validation accuracy and loss versus the minimum game cut off used, shown above.</p>

<p>The running average stats for each team become more robust as the cut off increases, which results in an improvement in the model’s performance on both the training and validation data sets. However, at a certain point, the training data size becomes too small, and the predictive power of the model starts to deteriorate, resulting in poorer performance on the validation data.</p>

<p>It’s interesting that the performance on the training data seems to somewhat plateau between the 10 and 30 game cut offs. Perhaps this suggests that there are two stages of stabilization: there is a lot of early season variation that flattens out quickly, followed by more long-term stabilization, perhaps related to lineup adjustment and trades. This seems challenging to capture with our model and would likely require more sophisticated time series modeling to account for.</p>

<p>In any case, just as our validation results predicts, the model trained using a 50 game cut off indeed performs best on the test data.</p>

<figure class="framed-figure">
  <img src="/assets/win_projector/four_factor_50.png" alt="season_four_50" />
  <figcaption>Logistic regression model based on season-to-date four factor stats (50 game cut off).</figcaption>
</figure>

<p>While the accuracy score matches our Bayes’ rate of ~69%, I’m not sure that imposing a 50 game cut off model really represents an improved model. The lower loss implies that the predictions are better calibrated, but only being able to predict games 51 through 82, as opposed to 11 through 82, is fairly constraining for only a 3% increase in the model accuracy from the 10 game cut off model. And since the sportsbooks’ predictions don’t require such a cut off, I’m hesitant to say this model matches their performance. To reach</p>

<h2 id="future-directions">Future Directions</h2>

<p>I had planned to explore the use of models more complex than logistic regression. Logistic regression is at its core a linear model, and it’s unlikely that wins and losses can be neatly separated by a linear decision boundary in our feature space. I trained a variety of different models to address this:</p>
<ul>
  <li>Random Forest Classifier</li>
  <li>Gradient Boosted Trees</li>
  <li>Support Vector Machine with various kernels</li>
  <li>Dense Neural Network with 1, 2, and 3 layers</li>
</ul>

<p>None of these models provided a noticeable advantage. My initial thought process was that there are nonlinear relationships between these features, and perhaps these more complex models can capture them. But this highlights the concept of a Bayes rate — if the experts can’t disentangle these relationships, the task is likely just too difficult.</p>

<p>Another possibility is that I’m missing some important latent variables. You can only extract the signal that is in the dataset, so if I’m missing some key pieces of this signal, my model is sure to underperform. Some potential features that can be added:</p>
<ul>
  <li>Injuries – Perhaps in the form of minutes per game lost due to injury.</li>
  <li>Strategy or roster changes — Adding the rolling average four factor stats over 5, 10, or 20 game windows would be a way to incorporate this temporal information.</li>
  <li>Individual matchup dynamics — One of the most important factors in a matchup is how the two play styles of the team’s fare against each other, but this information is difficult to both extract and encode numerically.</li>
</ul>

<p>I have already experimented with including rolling averages along as features and incorporating injury information, but more exploration is needed. Another future direction would be to granularize the model in some way. A tanking team and a team pushing for a playoff spot will approach a matchup very differently, and this could perhaps be captured by a hierarchical model.</p>

<h2 id="conclusions">Conclusions</h2>
<p>As a final note on this project, I want to compare the makeup of the incorrect and correct predictions made by my model and by Las Vegas. I will do so using a crude metric: the average magnitude of the spread. A matchup with a large absolute value of the spread represents a matchup where one team is heavily favored over the other, and both models should perform well. A spread close to zero corresponds to a tossup, and I expect both models to perform poorly. The results on the test data are shown in the table below (3,442 matchups):</p>

<p>As expected, the average magnitude of the spread is much closer to zero when the models disagree, 2.9, since this denotes that the classification problem is more difficult. I don’t think much can be taken from the difference in the average spread magnitude when my model is correct/Vegas is wrong and vice versa — this difference of 0.6 is small in comparison to the distributions of spreads within each group. But what’s more notable is the relatively larger average spread when both models are incorrect, 5.3. This suggests that the models aren’t misclassifying purely because the matchup is a tossup, but instead there are some important variables not being considered by both my model and the sportsbooks. With stronger feature engineering, I hope to be able to distill what exactly these variables are.</p>

<table style="border-collapse: collapse; text-align: right;">
  <tr>
    <th style="text-align: center;"></th>
    <th style="text-align: center;">Fraction</th>
    <th style="text-align: center;">Average |Spread|</th>
  </tr>
  <tr>
    <td style="text-align: left;">Model only correct</td>
    <td>5.6%</td>
    <td>2.6</td>
  </tr>
  <tr>
    <td style="text-align: left;">Vegas only correct</td>
    <td>6.9%</td>
    <td>3.2</td>
  </tr>
  <tr>
    <td style="text-align: left;">Both correct</td>
    <td>61.9%</td>
    <td>7.4</td>
  </tr>
  <tr>
    <td style="text-align: left;">Neither correct</td>
    <td>25.6%</td>
    <td>5.3</td>
  </tr>
  <!-- spacer row across all columns -->
  <tr>
    <td colspan="3" style="border-left: none; border-right: none; height: 10px;"></td>
  </tr>
  <tr>
    <td style="text-align: left;">Model &amp; Vegas Agree</td>
    <td>87.5%</td>
    <td>6.8</td>
  </tr>
  <tr>
    <td style="text-align: left;">Model &amp; Vegas Disagree</td>
    <td>12.5%</td>
    <td>2.9</td>
  </tr>
</table>

\[\sim\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[Can a machine learning model outperform Las Vegas in predicting NBA matchups? Using team gamelog stats, I built a model to forecast game outcomes and compare its performance against the sportsbooks' predictions.]]></summary></entry><entry><title type="html">Tarik Skubal is on a Historic Pace</title><link href="/k_bb_9/" rel="alternate" type="text/html" title="Tarik Skubal is on a Historic Pace" /><published>2025-07-10T00:00:00-04:00</published><updated>2025-07-10T00:00:00-04:00</updated><id>/k_bb_9</id><content type="html" xml:base="/k_bb_9/"><![CDATA[<p><img src="/assets/sports/top3_k_bb.png" alt="top3_k_bb_9" style="max-width: 48%; height: auto; float: left; padding-right: 50px" /></p>

<p>Tarik Skubal is on pace to be the first pitcher in MLB history to finish the season leading the majors in both strikeouts per nine innings (K/9) and walkers per nine innings (BB/9). Since integration, 1947, there have only been 7 pitchers who finished the season top three in both, shown in the graphic above. Two of them one the Cy Young award in their league (Martinez in 2000 and Burnes in 2021), three finished second (Schilling in 2001 and 2002 and Verlander in 2018), and somehow Max Scherzer finished fifth in 2015 (this was the crazy Arrieta vs. Greinke vs. Kershaw year).</p>

<p>Skubal is truly having a dominant season – it’ll be fun to see if he can keep this up!</p>

<p>(If you extend to the turn on the century, only four more names are added to this list: Walter Johnson, Cy Young, and Satchel Paige twice. But still, none lead the league in both!)</p>

\[\tilde{}\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[Skubal is on pace to become the first pitcher in modern history to lead his league in both K/9 and BB/9. Using the pybaseball library, I explored how his season compares to some of the most dominant of all time.]]></summary></entry><entry><title type="html">Evolution Olympic Swimming Gold Medal Times from 1912–2024</title><link href="/swimming/" rel="alternate" type="text/html" title="Evolution Olympic Swimming Gold Medal Times from 1912–2024" /><published>2025-07-01T00:00:00-04:00</published><updated>2025-07-01T00:00:00-04:00</updated><id>/swimming</id><content type="html" xml:base="/swimming/"><![CDATA[<figure style="text-align: center;">
  <div style="display: flex; gap: 10px; justify-content: center;">
    <img src="/assets/sports/w_100m_fr.png" alt="Women's 100m Freestyle" class="hover-zoom" style="max-width: 48%; height: auto;" />
    <img src="/assets/sports/m_100m_fr.png" alt="Men's 100m Freestyle" class="hover-zoom" style="max-width: 48%; height: auto;" />
  </div>
  <figcaption style="margin-top: 8px; font-size: 0.85em; color: #999;">
    (hover to enlarge)
  </figcaption>
</figure>

<p>Watching 22-year-old French swimmer Léon Marchand crush numerous records in the 2024 Paris Olympics was a snapshot of the evolution of the modern athlete. Improvements in sports medicine and health science, and the increase in the access to sports (or rather the decreases in segregation in sports) have all contributed to the smashing of records across all team and individual sports. Swimming is a example to examine further, simply because it has a long history in the Olympics. We have 28 Olympics to draw data from across over 100 years of history – a lot has changed</p>

<figure style="text-align: center;">
  <div style="display: flex; gap: 10px; justify-content: center;">
    <img src="/assets/sports/1912.png" alt="Women's 100m Freestyle" class="hover-zoom_small" style="max-width: 48%; height: auto;" />
    <img src="/assets/sports/2016.jpg" alt="Men's 100m Freestyle" class="hover-zoom_small" style="max-width: 48%; height: auto;" />
  </div>
  <figcaption style="margin-top: 8px; font-size: 0.85em; color: #999;">
    The evolution of the Olympic swimmer -- 1912 to 2016
  </figcaption>
</figure>

<p>I found the data for these plots on <a href="https://www.kaggle.com/datasets/datasciencedonut/olympic-swimming-1912-to-2020/data">Kaggle</a>, where someone had nicely collected the Gold winning times for nearly all swimming races from 1912-2020. I just had to add in 2024 by hand and plot the results using Python.</p>

<figure style="text-align: center;">
  <div style="display: flex; gap: 10px; justify-content: center;">
    <img src="/assets/sports/w_400m_fr.png" alt="Women's 100m Freestyle" class="hover-zoom" style="max-width: 48%; height: auto;" />
    <img src="/assets/sports/m_400m_fr.png" alt="Men's 100m Freestyle" class="hover-zoom" style="max-width: 48%; height: auto;" />
  </div>
  <figcaption style="margin-top: 8px; font-size: 0.85em; color: #999;">
    (hover to enlarge)
  </figcaption>
</figure>

\[\tilde{}\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[Athletes have gotten bigger, stronger, and smarter &mdash; and the Olympics provides a perfect stage to showcase this. Plotting the gold-medal winning times in two of the most iconic swimming events, we clearly see the impact of sports science on athelete performance.]]></summary></entry><entry><title type="html">Evolution of the NBA Shot Chart</title><link href="/shot_chart/" rel="alternate" type="text/html" title="Evolution of the NBA Shot Chart" /><published>2025-05-30T00:00:00-04:00</published><updated>2025-05-30T00:00:00-04:00</updated><id>/shot_chart</id><content type="html" xml:base="/shot_chart/"><![CDATA[<div style="text-align: center;">
<video width="640" height="360" controls="" autoplay="" loop="" muted="" playsinline="">
  <source src="/assets/sports/hm_gif.webm" type="video/webm" />
  Your browser does not support the video tag.
</video>
<p style="font-size: 14px; color: #666; margin-top: 5px;">
    Evolution of the NBA jump shot heatmap. Demar Derozan must be lonely on the deep-two's islands!
  </p>
</div>

<p>I was sent a link to a <a href="https://github.com/DomSamangy/NBA_Shots_04_25">GitHub repo</a> containing every shot attempted in the NBA from the 2004 to 2024 season, and felt the gears start turning in my head. With such a rich, dense set of data, there was sure to be some insights to be gained. The data was stored in CSV files, one per season, which made loading, processing, and visualizing the year by year data difficult. Seeking a better approach, I was led to <a href="https://github.com/mpope9/nba-sql"><code class="language-plaintext highlighter-rouge">nba-sql</code></a>, a Python based library that can be used to scrape all sorts of data from stats.nba.com. What was appealing to me about this library was that the data could be stored in an SQLite relational database. In addition to being better equipped to handle and query this large quantity of data, this approach also provided a nice playground to brush up on my SQL and learn about object-relational mapping (ORM). You can read about my ORM journey at the end of the post.</p>

<p>To generate the animation above, I scraped the entire shot chart data base for each year from 1997-98 to 2024-25, which required some minor modifications to the <code class="language-plaintext highlighter-rouge">nba-sql</code> library (at first, only the most recent seasons shot chart data was accessible). To isolate jumpers, I filtered out all layups and dunks by removing any attempts in the restricted area. This likely included some non-jump shot attempts, just outside the resricted area (e.g. floaters), but was more reliable than using the shot descriptions included in the dataset (the descriptions included fluctuated dramatically year-to-year). I then generated heatmaps for each year, stitching the results into an animation showing the evoluation over the past ~30 years.</p>

<p>I found the gradual formation of the “deep-two’s” islands interesting. What once were hotspots for shot attemps in the 90’s became nearly compltely removed from the game in the 20’s (save for Demar Derozan, our mid range king). These shots have of course migrated to the three point line, but have also trickled into the paint. We can analyze this more carefully by breaking down the shot attempts by zone.</p>

<h2 id="jump-shot-attempts-by-zone">Jump Shot Attempts by Zone</h2>

<p><img src="/assets/sports/jump_shot_zone.png" alt="jumper" /></p>

<p>The share of jump shots attempted in the mid range (blue) has shrunk from nearly 55% in 1997 to just 14% in 2024! Meanwhile three attempts (green) have increased from 24% to 58%, and paint shots (red) from 21% to 28%.</p>

<p>Another way to visualize this trend is through the average shot distance over time, shown in the bottom right panel. From 1999 to 2019, the average shot distance increased by 3 feet, which one might be tempted to attribute to the substitution of deep twos (~22 footers) for threes (~25 footer). But it’s not that simple, and I think the sharp dip in average shot distance around 2010 highlights this. Shot distance is much more granular than jump shot zone distribution and better captures the slow change in basketball philosophy that occured during the past two decades.</p>

<h2 id="jump-shot-attempts-by-distance">Jump Shot Attempts by Distance</h2>

<p>My first thought was that the dip in average shot distance could be result of the embrace of the corner three. Maybe teams and players were beginning to acknowledge that 3&gt;2, but were not yet ready to accept that if you practice enough, you can shoot a high volume of 25 footers at a reasonable clip. The corner three would be like the gateway drug, its a closer shot than an above the break three, but still takes full advtange of the fact that more points is better.</p>

<p><img src="/assets/sports/corner.png" alt="corner" /></p>

<p>This was not borne out in the data. While distribution of threes showed a small tilt towards corner shots, this trend plateaued by the mid-2000’s. In fact, there is actually a dip in the share of threes shot from the corners around 2016, which is right around the time when “deep three’s” entered the playbook.</p>

<p><img src="../assets/gif/double_bang.gif" alt="Bang Bang" style="display:block; margin-left:auto; margin-right:auto" /></p>

<p><img src="/assets/sports/attempts_by_dist.png" alt="attempts_by_dist" /></p>

<p>I then broke down the yearly distrubition of shot attempts by distance, splitting into 4 foot ranges (4-8 ft., 8-12 ft., etc.). These ranges are depicted in the upper left inset of the above plot. The dip in the average shot distance around 2010 can be attributed to a decrease in the number of deep two’s taken (blue, 16-20 ft.), coupled with an increase in the number shots near the rim (red, 4-8 ft.) and a relative plateau in the number of threes attempted (green, 24+ ft.). So it seems that the mid range was on its decline, but the three point revolution had not yet fully commenced.</p>

<p>I think that this embrace of the paint jumper can more or less be tied to the decline of the big man. Looking at the top shot attempters in this distance range by year, the 90’s and early 00’s were dominated by bigs (Shaq and Duncan), but by 2010, we start to see more big guards and forwards (Kobe and Shawn Marion crop up in the top 5). In this current area, the list of top shooters in the 4-8 ft. range is <em>filled</em> with big guards, like Harden, Luka, Cade Cunningham, and Josh Giddey. As the paint got less clogged, more shots opened up close to the basket, and players started to take advantage.</p>

<p><img src="/assets/sports/height_hm.png" alt="height" /></p>

<p>I look foward to playing around more with this dataset and the others accessible using <code class="language-plaintext highlighter-rouge">nba-sql</code>!</p>

<h2 id="nba_api"><code class="language-plaintext highlighter-rouge">nba_api</code></h2>
<p>The first task was to get familiar with the <code class="language-plaintext highlighter-rouge">nba-sql</code> library. This library draws quite a lot from the popular <a href="https://github.com/swar/nba_api"><code class="language-plaintext highlighter-rouge">nba_api</code></a> Python library, which I have used for simple queries over the years. Both use the Python <code class="language-plaintext highlighter-rouge">requests</code> package to scrape the NBA’s stats webpages for data (from “endpoints” - undocumented locations within stats.nba.com containing undocumented sets of data), but <code class="language-plaintext highlighter-rouge">nba_api</code> simply returns the results of these queries as pandas DataFrames. This is really only helpful for lightweight data analysis, highlighting the main purpose of <code class="language-plaintext highlighter-rouge">nba_api</code>:</p>
<blockquote>
  <p>A significant purpose of this package is to continuously map and analyze as many endpoints on NBA.com as possible. The documentation and analysis of the endpoints and parameters in this package are some of the most extensive information available. At the same time, NBA.com does not provide information regarding new, changed, or removed endpoints.</p>
</blockquote>

<h2 id="nba-sql"><code class="language-plaintext highlighter-rouge">nba-sql</code></h2>
<p>The <code class="language-plaintext highlighter-rouge">nba-sql</code> library allows you to scrape data from ten different endpoints:</p>
<ul>
  <li>player_season</li>
  <li>player_game_log</li>
  <li>play_by_play</li>
  <li>play_by_playv3</li>
  <li>pgtt</li>
  <li>shot_chart_detail</li>
  <li>game</li>
  <li>event_message_type</li>
  <li>team</li>
  <li>player</li>
</ul>

<p>In all cases, the general framework of collecting the data is the same:</p>
<ul>
  <li>Determine a set of parameters for the query (e.g. a list of game id’s number for player_game_log or a collection of (player id, team id, season) trios for short_chart_detail).</li>
  <li>Use the <code class="language-plaintext highlighter-rouge">requests</code> package to <code class="language-plaintext highlighter-rouge">get</code> the data contained in the endpoint. The previously determined parameters, along with some predefined defaults, are fed into the <code class="language-plaintext highlighter-rouge">get</code> function, which essentially tacks them on in a specific way to the end of stats.nba.com/stats/{endpoint}/ before accessing the webpage. A “header” is also passed along in the request, which is a list of settings that are used to trick stats.nba.com into thinking that the site is being visited by a human using a browser, rather than a data-scraping bot. My understanding is that many websites that host data are okay with scraping, as long as some guidelines are met (minimum time between requests, for example). But if you aren’t careful, yor IP address can be blocked from accessing a site (this has happened to me with baseball-reference many times…).</li>
  <li>The data is generally stored in some type of table. After identifying the columns of interest (this is where <code class="language-plaintext highlighter-rouge">nba_api</code> comes in), the desired values from each row in the table are stored in an array.</li>
  <li>This array (list of data rows) is then <code class="language-plaintext highlighter-rouge">populate</code>d into an SQLite database <code class="language-plaintext highlighter-rouge">nba_sql.db</code>. This is done using the python library <a href="https://docs.peewee-orm.com/en/latest/">peewee</a>. This is my first experience using an ORM - in my <a href="/contracts/">previous data scraping projects</a>, the datasets were either small or the data structures were simple enough to use flat CSV files. An ORM allows you to use Python object oriented programing principles to create, insert data into, and modify SQL databases. This is particularly useful for relationship databases, where the rows of the various tables are related to each other using keys (player id, team id, and game id being examples). In <code class="language-plaintext highlighter-rouge">nba-sql</code> a selection of tables (peewee <code class="language-plaintext highlighter-rouge">Model</code> objects), corresponding to stats.nba.com endpoints, have been defined with their corresponding fields. During the <code class="language-plaintext highlighter-rouge">populate</code> stage, the data is inserted into these tables (<code class="language-plaintext highlighter-rouge">model</code>s), which are bound to a local <code class="language-plaintext highlighter-rouge">nba_sql.db</code> database file. This file can then be queried as pleased.</li>
</ul>

\[\tilde{}\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[The rise of the three point shot in the NBA is well documented and well understood. In this post, I explore this trend in further detail using shot chart data scraped from the NBA Stats website and managed using the peewee Python ORM.]]></summary></entry><entry><title type="html">Distribution of Annual Salaries and Total Contract Values for each of the Big Four Sports Leagues</title><link href="/contracts/" rel="alternate" type="text/html" title="Distribution of Annual Salaries and Total Contract Values for each of the Big Four Sports Leagues" /><published>2025-01-15T00:00:00-05:00</published><updated>2025-01-15T00:00:00-05:00</updated><id>/contracts</id><content type="html" xml:base="/contracts/"><![CDATA[<p><img src="/assets/sports/salaries.png" alt="Salaries" /></p>

<p>With the signing of Juan Soto’s record setting 15 year, $765 million contract, I was curious how this number compared to the rest of the salaries in the MLB, as well as those across each of the Big Four American sports.</p>

<p>The contract information for each MLB, NBA, NFL, and NHL player is available on <a href="https://www.spotrac.com/">https://www.spotrac.com/</a>. To create these plots, I scraped the site using the Python library <code class="language-plaintext highlighter-rouge">BeautifulSoup</code> and collected all individual player AAV’s and contract lengths. I used these to create histograms, which I have plotted as smooth kernel density estimations.</p>

<p>It’s fascinating how different the spread of AAV’s are in each league. The NBA’s is starkly different from the rest, with a long tail and a broad belly. The NBA is the smallest of the four leagues, as well as the leauge where individual players have the most impact. Perhaps this creates a market where it can be justified to give any contract to anybody? Fred VanVleet could organize our young core? $43 million should do! Jonathan Issac has DPOY potential? $25 million! Brandon Ingram is a proffesional scorer? $36 million! John Collins jumped over a plane in a dunk contest? $27 million!
If you squint your eyes, you can see why X team gave out Y contract to Z player, even if its looks horrible in hindsight (well, except for a <a href="https://en.wikipedia.org/wiki/Timofey_Mozgov#Los_Angeles_Lakers_(2016%E2%80%932017)">few</a>). In the NBA it can be worth it to take the risk becuase sometimes, something will click, and that bad contract may actually be just fine.</p>

<p><img src="../assets/gif/wiggins.gif" alt="Forever greatful" style="display:block; margin-left:auto; margin-right:auto" /></p>

<p>We can contrast that with the MLB, where the most widely used Sabermetric stat is literally called wins above <em>replacement</em>. There are so many players than can do an alright enough job that to really value a player, you need to compare them to what the <a href="https://www.mlb.com/glossary/advanced-stats/wins-above-replacement">MLB defines as</a> as “a Minor League replacement or a readily available fill-in free agent.” So someone not in the major leagues. Baseball is just as much about winning on the margins as it is about rostering the highest profile, highest paid stats (didn’t someone write a book about this?). As a result, it’s harder to justify giving out big money to someone who just may play at an All Star level; better to spread that money out a amongst your roster so that you don’t put out an infield of replacement players.</p>

<p><img src="/assets/sports/contract_values.png" alt="Contract Values" /></p>

<p>The distribution of total contract values is a different story. There are a ton of high AAV contracts in the NBA, but they just aren’t that long. As a result, the broad belly seen in the distribution of average annual salaries doesn’t stand out here. The MLB, and the NFL to some extent, are composed mostly of short contracts with (relatively)small AAV’s, so the total contract values are pretty sharply peaked below $50 million. But there are some clear outlier large constracts in these two leagues. Soto and Ohtani headline the list, but Mike Trout was really the first to do it. What strikes me about these three contracts is that no one really thinks of these are overpays. I think you can say the same thing about Patrick Mahomes. The “stars and studs” nature of roster building in the MLB and NFL means that only the elite of the elite get these mind-blowingly large contracts (unlike in the NBA, if you were to ask someone outside of Boston).</p>

\[\tilde{}\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[Why do the types of contracts given out in each of the big four men's sports leagues differ so much? I explored this by looking at the distribution of salaries (annual and total) across all players in each league.]]></summary></entry><entry><title type="html">Phil Jackson’s “40 Wns Before 20 Loses” Rule</title><link href="/pj/" rel="alternate" type="text/html" title="Phil Jackson’s “40 Wns Before 20 Loses” Rule" /><published>2020-02-26T00:00:00-05:00</published><updated>2020-02-26T00:00:00-05:00</updated><id>/pj</id><content type="html" xml:base="/pj/"><![CDATA[<!-- <p align="center">
  <img width="75%" src="/assets/sports/PJ_graphic.jpg">
</p> -->

<p><img src="/assets/sports/PJ_graphic.jpg" alt="40_20" style="max-width: 60%; height: auto; float: left; padding-right: 50px" /></p>

<p>Legendary NBA coach Phil Jackson had a “40/20” rule: if you win 40 games before you lose 20, you are officially a contender to win the NBA title. I wanted to see if there was anything behind this, so I scraped <a href="https://www.basketball-reference.com/">Basketball Reference</a> and compiled the end of season outcome for every 40+ win team since the NBA-ABA merger (1978-1979 season).</p>

<p>Only 4 teams have ever won the NBA Finals after losing their 20th game before winning their 40th (the 1979 Seattle Super Sonics, 1995 Houston Rockets, 2004 Detroit Pistons, 2006 Miami Heat)!</p>

<p><br /><br /><br /></p>

<p>Edit: This graphic was made in 2020, since then one more team has won the finals while breaking Phil Jackson’s rule, the 2021 Milwaukee Bucks.</p>

<p>Edit 2: In 2025 this still holds up! No team since the Bucks in 2021 have won the finals while breaking this rule.</p>

\[\tilde{}\]]]></content><author><name>Gabe Schumm</name><email>gabes135  at gmail  dot com</email></author><summary type="html"><![CDATA[The very first sports infographic I ever made! A simple test of the concept that in the NBA, a team must win 40 games before losing 20 to be a real contender for the championship.]]></summary></entry></feed>